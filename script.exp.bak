#!/usr/bin/expect -f
#
# This Expect script was generated by autoexpect on Mon Sep  9 15:07:30 2019
# Expect and autoexpect were both written by Don Libes, NIST.
#
# Note that autoexpect does not guarantee a working script.  It
# necessarily has to guess about certain things.  Two reasons a script
# might fail are:
#
# 1) timing - A surprising number of programs (rn, ksh, zsh, telnet,
# etc.) and devices discard or ignore keystrokes that arrive "too
# quickly" after prompts.  If you find your new script hanging up at
# one spot, try adding a short sleep just before the previous send.
# Setting "force_conservative" to 1 (see below) makes Expect do this
# automatically - pausing briefly before sending each character.  This
# pacifies every program I know of.  The -c flag makes the script do
# this in the first place.  The -C flag allows you to define a
# character to toggle this mode off and on.

set force_conservative 0  ;# set to 1 to force conservative mode even if
			  ;# script wasn't run conservatively originally
if {$force_conservative} {
	set send_slow {1 .1}
	proc send {ignore arg} {
		sleep .1
		exp_send -s -- $arg
	}
}

#
# 2) differing output - Some programs produce different output each time
# they run.  The "date" command is an obvious example.  Another is
# ftp, if it produces throughput statistics at the end of a file
# transfer.  If this causes a problem, delete these patterns or replace
# them with wildcards.  An alternative is to use the -p flag (for
# "prompt") which makes Expect only look for the last line of output
# (i.e., the prompt).  The -P flag allows you to define a character to
# toggle this mode off and on.
#
# Read the man page for more info.
#
# -Don


set timeout -1
spawn ./configure
match_max 100000
expect "Please specify the location of python."
send -- "/usr/bin/python3\r"
expect "Please input the desired Python library path to use."
send -- "\r"
expect -exact "\r
Do you wish to build TensorFlow with XLA JIT support? \[Y/n\]: "
send -- "n\r"
expect -exact "n\r
No XLA JIT support will be enabled for TensorFlow.\r
\r
Do you wish to build TensorFlow with OpenCL SYCL support? \[y/N\]: "
send -- "n\r"
expect -exact "n\r
No OpenCL SYCL support will be enabled for TensorFlow.\r
\r
Do you wish to build TensorFlow with ROCm support? \[y/N\]: "
send -- "n\r"
expect -exact "n\r
No ROCm support will be enabled for TensorFlow.\r
\r
Do you wish to build TensorFlow with CUDA support? \[y/N\]: "
send -- "y\r"
expect -exact "y\r
CUDA support will be enabled for TensorFlow.\r
\r
Do you wish to build TensorFlow with TensorRT support? \[y/N\]: "
send -- "y\r"
expect -exact "y\r
TensorRT support will be enabled for TensorFlow.\r
\r
Could not find any NvInferVersion.h matching version '' in any subdirectory:\r
        ''\r
        'include'\r
        'include/cuda'\r
        'include/*-linux-gnu'\r
        'extras/CUPTI/include'\r
        'include/cuda/CUPTI'\r
of:\r
        '/lib/x86_64-linux-gnu'\r
        '/usr'\r
        '/usr/lib/x86_64-linux-gnu'\r
        '/usr/local/cuda'\r
        '/usr/local/cuda-10.0/targets/x86_64-linux/lib'\r
Asking for detailed CUDA configuration...\r
\r
Please specify the CUDA SDK version you want to use. \[Leave empty to default to CUDA 10\]: "
send -- "\r"
expect -exact "\r
\r
\r
Please specify the cuDNN version you want to use. \[Leave empty to default to cuDNN 7\]: "
send -- "\r"
expect -exact "\r
\r
\r
Please specify the TensorRT version you want to use. \[Leave empty to  default to TensorRT 7\]: "
send -- "\r"
expect -exact "\r
\r
\r
Please specify the locally installed NCCL version you want to use. \[Leave empty to use http://github.com/nvidia/nccl\]: "
send -- "\r"
expect -exact "\r
\r
\r
Please specify the comma-separated list of base paths to look for CUDA libraries and headers. \[Leave empty to use the default\]: "
send -- "/usr/local/cuda/,/usr/local/cuda/lib64,/usr/local/cuda/include,/opt/TensorRT-7.0.0.11/lib,/opt/TensorRT-7.0.0.11/include,/usr/include,/usr/lib,/usr/lib/x86_64-linux-gnu"
expect -exact "/usr/local/cuda/,/usr/local/cuda/lib64,/usr/local/cuda/include,/opt/TensorRT-7.0.0.11/lib,/opt/TensorRT-7.0.0.11/include,/usr/include,/usr/lib,/usr/lib/x86_64-linux-gnu"
send -- "\r"
expect "Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 \[Default is: 3.5,7.0\]: "
send -- "3.5,7.0,7.5\r"
expect -exact "3.5,7.0,7.5\r
\r
\r
Do you want to use clang as CUDA compiler? \[y/N\]: "
send -- "\r"
expect -exact "\r
nvcc will be used as CUDA compiler.\r
\r
Please specify which gcc should be used by nvcc as the host compiler. \[Default is /usr/bin/gcc\]: "
send -- "\r"
expect -exact "\r
\r
\r
Do you wish to build TensorFlow with MPI support? \[y/N\]: "
send -- "\r"
expect -exact "\r
No MPI support will be enabled for TensorFlow.\r
\r
Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified \[Default is -march=native -Wno-sign-compare\]: "
send -- "\r"
expect -exact "\r
\r
\r
Would you like to interactively configure ./WORKSPACE for Android builds? \[y/N\]: "
send -- "\r"
expect eof
